{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (512, 1024)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"../dataset/ucsf-idl-resized/images/ffgh0257/page_0.jpg\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from torchvision.transforms.functional import resize, rotate\n",
    "from torchvision import transforms\n",
    "from PIL import ImageOps, Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ImageProcessor:\n",
    "    def __init__(self, img_size:tuple = (224,224)):\n",
    "        self.input_size = img_size\n",
    "        self.align_long_axis = False\n",
    "        self.random_padding = False\n",
    "\n",
    "        self.to_tensor = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def prepare_input(self, img: Image.Image, random_padding: bool = False) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        img = img.convert(\"RGB\")\n",
    "        if self.align_long_axis and (\n",
    "            (self.input_size[0] > self.input_size[1] and img.height > img.width)\n",
    "            or (self.input_size[0] < self.input_size[1] and img.height < img.width)\n",
    "        ):\n",
    "            img = rotate(img, angle=-90, expand=True)\n",
    "        img = resize(img, min(self.input_size))\n",
    "        img.thumbnail((self.input_size[0], self.input_size[1]))\n",
    "        delta_width = self.input_size[0] - img.width\n",
    "        delta_height = self.input_size[1] - img.height\n",
    "        if random_padding:\n",
    "            pad_width = np.random.randint(low=0, high=delta_width + 1)\n",
    "            pad_height = np.random.randint(low=0, high=delta_height + 1)\n",
    "        else:\n",
    "            pad_width = delta_width // 2\n",
    "            pad_height = delta_height // 2\n",
    "        padding = (\n",
    "            pad_width,\n",
    "            pad_height,\n",
    "            delta_width - pad_width,\n",
    "            delta_height - pad_height,\n",
    "        )\n",
    "\n",
    "        pixel_values = torch.Tensor(self.to_tensor(ImageOps.expand(img, padding)))\n",
    "        return {\"pixel_values\": pixel_values.unsqueeze(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = ImageProcessor(img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = image_processor.prepare_input(img)\n",
    "inputs[\"pixel_values\"].size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwinEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SwinModel, SwinConfig\n",
    "\n",
    "config = SwinConfig(\n",
    "    image_size=img_size,\n",
    "    depths=[2, 2, 14, 2],\n",
    "    window_size=8,\n",
    "    patch_size=8,\n",
    "    embed_dim=128,\n",
    "    num_heads=[4, 8, 16, 32],\n",
    "    num_classes=0\n",
    ")\n",
    "swin_encoder = SwinModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_encoder.config.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = swin_encoder(**inputs)\n",
    "embeddings.last_hidden_state.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tchibo-id_processing-wXqe-H2F",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ce36a046d53e0aa778d8b7cb3073fcbd842a253eb36f7f1df9b60e887a5bafd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
