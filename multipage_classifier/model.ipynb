{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (512, 1024)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = \"doc_2.jpg\"\n",
    "img = Image.open(\"../dataset/samples/\" + img_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannesscheibe/Library/Caches/pypoetry/virtualenvs/multipage-classifier-DHgYOAvi-py3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from torchvision.transforms.functional import resize, rotate\n",
    "from torchvision import transforms\n",
    "from PIL import ImageOps, Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ImageProcessor:\n",
    "    def __init__(self, img_size:tuple = (224,224)):\n",
    "        self.input_size = img_size\n",
    "        self.align_long_axis = False\n",
    "        self.random_padding = False\n",
    "\n",
    "        self.to_tensor = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def prepare_input(self, img: Image.Image, random_padding: bool = False) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        img = img.convert(\"RGB\")\n",
    "        if self.align_long_axis and (\n",
    "            (self.input_size[0] > self.input_size[1] and img.height > img.width)\n",
    "            or (self.input_size[0] < self.input_size[1] and img.height < img.width)\n",
    "        ):\n",
    "            img = rotate(img, angle=-90, expand=True)\n",
    "        img = resize(img, min(self.input_size))\n",
    "        img.thumbnail((self.input_size[0], self.input_size[1]))\n",
    "        delta_width = self.input_size[0] - img.width\n",
    "        delta_height = self.input_size[1] - img.height\n",
    "        if random_padding:\n",
    "            pad_width = np.random.randint(low=0, high=delta_width + 1)\n",
    "            pad_height = np.random.randint(low=0, high=delta_height + 1)\n",
    "        else:\n",
    "            pad_width = delta_width // 2\n",
    "            pad_height = delta_height // 2\n",
    "        padding = (\n",
    "            pad_width,\n",
    "            pad_height,\n",
    "            delta_width - pad_width,\n",
    "            delta_height - pad_height,\n",
    "        )\n",
    "\n",
    "        pixel_values = torch.Tensor(self.to_tensor(ImageOps.expand(img, padding)))\n",
    "        return {\"pixel_values\": pixel_values.unsqueeze(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = ImageProcessor(img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = image_processor.prepare_input(img)\n",
    "inputs[\"pixel_values\"].size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwinEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SwinModel, SwinConfig\n",
    "\n",
    "config = SwinConfig(\n",
    "    image_size=img_size,\n",
    "    depths=[2, 2, 14, 2],\n",
    "    window_size=8,\n",
    "    patch_size=8,\n",
    "    embed_dim=128,\n",
    "    num_heads=[4, 8, 16, 32],\n",
    "    num_classes=0\n",
    ")\n",
    "swin_encoder = SwinModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_encoder.config.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = swin_encoder(**inputs)\n",
    "embeddings.last_hidden_state.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipage-classifier-DHgYOAvi-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "907d734be3f332e74f26e7a3f49b4ab8ae03974515266479bd8e13e854b6d5f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
