{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/training/pypoetry/virtualenvs/multipage-classifier-training-o8JTDpqF-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "from visual_page_classifier.lightning_module import VisualPageClassifierPLModule, VisualPageClassifier\n",
    "from multipage_transformer.lightning_module import MultipageTransformerPLModule, MultipageTransformer\n",
    "from page_comparsion_encoder.lightning_module import PageComparisonEncoderPLModule, PageComparisonEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MBartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'decoder.layer_norm.weight', 'decoder.layer_norm.bias', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of MBartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized because the shapes did not match:\n",
      "- decoder.embed_positions.weight: found shape torch.Size([1026, 768]) in the checkpoint and torch.Size([770, 1024]) in the model instantiated\n",
      "- decoder.layernorm_embedding.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layernorm_embedding.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.encoder_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- decoder.layers.0.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([4096, 1024]) in the model instantiated\n",
      "- decoder.layers.0.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([1024, 4096]) in the model instantiated\n",
      "- decoder.layers.0.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.0.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.encoder_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- decoder.layers.1.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([4096, 1024]) in the model instantiated\n",
      "- decoder.layers.1.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([1024, 4096]) in the model instantiated\n",
      "- decoder.layers.1.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.1.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.encoder_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- decoder.layers.2.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([4096, 1024]) in the model instantiated\n",
      "- decoder.layers.2.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([1024, 4096]) in the model instantiated\n",
      "- decoder.layers.2.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.2.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.encoder_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- decoder.layers.3.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([4096, 1024]) in the model instantiated\n",
      "- decoder.layers.3.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([1024, 4096]) in the model instantiated\n",
      "- decoder.layers.3.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([1024, 1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- decoder.layers.3.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "VISUAL_CLASSIFIER_MODEL_PATH = \"/data/training/master_thesis/evaluation_logs/visual_page_classifier/version_1/checkpoints/best-checkpoint.ckpt\"\n",
    "DUAL_CLASSIFIER_MODEL_PATH = \"/data/training/master_thesis/evaluation_logs/page_comparision_encoder/microsoft/swinv2-base-patch4-window8-256/version_0/checkpoints/best-checkpoint.ckpt\"\n",
    "TRANSFORMER_MODEL_PATH = \"/data/training/master_thesis/evaluation_logs/multipage_transformer/version_1/checkpoints/best-checkpoint.ckpt\"\n",
    "\n",
    "# Load Model\n",
    "visual_classifier: VisualPageClassifier = VisualPageClassifierPLModule.load_from_checkpoint(VISUAL_CLASSIFIER_MODEL_PATH, map_location=\"cpu\").eval().classifier\n",
    "dual_classifier: PageComparisonEncoder = PageComparisonEncoderPLModule.load_from_checkpoint(DUAL_CLASSIFIER_MODEL_PATH, map_location=\"cpu\").eval().model\n",
    "transformer: MultipageTransformer = MultipageTransformerPLModule.load_from_checkpoint(TRANSFORMER_MODEL_PATH, map_location=\"cpu\").eval().model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PATH = \"/data/training/master_thesis/datasets/evaluation/ma_sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_PATH = \"/data/training/master_thesis/datasets/bzuf_classes.json\"\n",
    "classes = [c for c in json.load(open(CLASS_PATH))]\n",
    "id2class = {idx: str(label) for idx, label in enumerate(classes)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "folder_path = Path(SAMPLE_PATH)  # Replace with the path to your folder\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = [file for file in os.listdir(folder_path) if file.endswith(\".png\")]\n",
    "\n",
    "# Sort the files based on page number\n",
    "file_list.sort(key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "\n",
    "# Initialize a list to store the loaded images\n",
    "images = []\n",
    "\n",
    "# Load each image using PIL\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    image = Image.open(file_path)\n",
    "    images.append(image)\n",
    "\n",
    "ground_truth = json.load(open(folder_path / \"ground_truth.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/training/pypoetry/virtualenvs/multipage-classifier-training-o8JTDpqF-py3.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def visual_classifier_inference(images: list[Image.Image]):\n",
    "    # prepare input\n",
    "    pixel_values = torch.cat([visual_classifier.encoder.page_encoder.prepare_input(img).unsqueeze(0) for img in images])\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        output = visual_classifier.predict(pixel_values)\n",
    "\n",
    "    doc_id_pred = [p.item() for p in output[\"doc_id\"]]\n",
    "    class_pred = [id2class[int(p.item())] for p in output[\"doc_class\"].argmax(1)]\n",
    "    page_nr_pred = [p.item() for p in output[\"page_nr\"].argmax(1)]\n",
    "\n",
    "    return doc_id_pred, class_pred, page_nr_pred\n",
    "\n",
    "def dual_classifier_inference(images: list[Image.Image]):\n",
    "    # prepare input\n",
    "    pixel_values = torch.cat([dual_classifier.encoder.prepare_input(img).unsqueeze(0) for img in images])\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        output = dual_classifier.forward(pixel_values)\n",
    "        output = dual_classifier.postprocess(output)\n",
    "\n",
    "    doc_id_pred = [p.item() for p in output[\"doc_id\"]]\n",
    "    class_pred = [id2class[int(p.item())] for p in output[\"doc_class\"].argmax(1)]\n",
    "    page_nr_pred = [p.item() for p in output[\"page_nr\"]]\n",
    "\n",
    "    return doc_id_pred, class_pred, page_nr_pred\n",
    "\n",
    "def transformer_inference(images: list[Image.Image]):\n",
    "    # prepare input\n",
    "    pixel_values = torch.cat([transformer.encoder.page_encoder.prepare_input(img).unsqueeze(0) for img in images])\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():  \n",
    "        output = transformer.predict(pixel_values)\n",
    "\n",
    "    doc_id_pred = []\n",
    "    class_pred = []\n",
    "    page_nr_pred = []\n",
    "    def extract_pred(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            doc_id_pred.append(obj.get(\"doc_id\", \"N/A\"))\n",
    "            class_pred.append(obj.get(\"doc_class\", \"N/A\"))\n",
    "            page_nr_pred.append(obj.get(\"page_nr\", \"N/A\"))\n",
    "            return \n",
    "        doc_id_pred.append(-1)\n",
    "        class_pred.append(-1)\n",
    "        page_nr_pred.append(-1)\n",
    "\n",
    "    if isinstance(output, list):\n",
    "        for elem in output:\n",
    "            extract_pred(elem)\n",
    "    elif isinstance(output, dict):\n",
    "        extract_pred(output)\n",
    "\n",
    "    doc_id_pred = doc_id_pred + [\"N/A\"] * (len(pixel_values) -len(doc_id_pred)) \n",
    "    class_pred = class_pred + [\"N/A\"] * (len(pixel_values) -len(class_pred)) \n",
    "    page_nr_pred = page_nr_pred + [\"N/A\"] * (len(pixel_values) -len(page_nr_pred)) \n",
    "    \n",
    "    return doc_id_pred, class_pred, page_nr_pred\n",
    "\n",
    "v_id, v_class, v_nr = visual_classifier_inference(images)\n",
    "d_id, d_class, d_nr = dual_classifier_inference(images)\n",
    "t_id, t_class, t_nr = transformer_inference(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 &  &  & \\\\\n",
      "Ground Truth & 0 & antrag-formlos & 0\\\\\n",
      "PageComparisionEncoder & 0 & antrag-formlos & 0 \\\\\n",
      "VisualPageClassifier & 0 & anschreiben & 0 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 2 &  &  & \\\\\n",
      "Ground Truth & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 0\\\\\n",
      "PageComparisionEncoder & 1 & antrag-formblattantrag-heilpaedagogische\\_tagesstaette & 1 \\\\\n",
      "VisualPageClassifier & 1 & antrag-formblattantrag-ambulant\\_betreutes\\_wohnen & 0 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 3 &  &  & \\\\\n",
      "Ground Truth & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 1\\\\\n",
      "PageComparisionEncoder & 1 & antrag-formblattantrag-heilpaedagogische\\_tagesstaette & 0 \\\\\n",
      "VisualPageClassifier & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 1 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 4 &  &  & \\\\\n",
      "Ground Truth & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 2\\\\\n",
      "PageComparisionEncoder & 1 & antrag-formblattantrag-heilpaedagogische\\_tagesstaette & 1 \\\\\n",
      "VisualPageClassifier & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 2 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 5 &  &  & \\\\\n",
      "Ground Truth & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 3\\\\\n",
      "PageComparisionEncoder & 1 & antrag-formblattantrag-heilpaedagogische\\_tagesstaette & 2 \\\\\n",
      "VisualPageClassifier & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 3 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 6 &  &  & \\\\\n",
      "Ground Truth & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 4\\\\\n",
      "PageComparisionEncoder & 1 & antrag-formblattantrag-heilpaedagogische\\_tagesstaette & 3 \\\\\n",
      "VisualPageClassifier & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 4 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 7 &  &  & \\\\\n",
      "Ground Truth & 1 & antrag-formblattantrag-hilfe\\_zur\\_pflege & 5\\\\\n",
      "PageComparisionEncoder & 1 & antrag-formblattantrag-heilpaedagogische\\_tagesstaette & 4 \\\\\n",
      "VisualPageClassifier & 1 & antrag-formblattantrag-ambulant\\_betreutes\\_wohnen & 5 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 8 &  &  & \\\\\n",
      "Ground Truth & 2 & vermoegen-vermoegenserklaerung & 0\\\\\n",
      "PageComparisionEncoder & 2 & vermoegen-vermoegenserklaerung & 5 \\\\\n",
      "VisualPageClassifier & 2 & vermoegen-vermoegenserklaerung & 0 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 9 &  &  & \\\\\n",
      "Ground Truth & 2 & vermoegen-vermoegenserklaerung & 1\\\\\n",
      "PageComparisionEncoder & 2 & vermoegen-vermoegenserklaerung & 0 \\\\\n",
      "VisualPageClassifier & 2 & vermoegen-vermoegenserklaerung & 1 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 10 &  &  & \\\\\n",
      "Ground Truth & 3 & vermoegen-erklaerung\\_zur\\_vermoegensverwendung & 0\\\\\n",
      "PageComparisionEncoder & 3 & vermoegen-erklaerung\\_zur\\_vermoegensverwendung & 1 \\\\\n",
      "VisualPageClassifier & 3 & vermoegen-kontoauszug & 0 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 11 &  &  & \\\\\n",
      "Ground Truth & 3 & vermoegen-erklaerung\\_zur\\_vermoegensverwendung & 1\\\\\n",
      "PageComparisionEncoder & 3 & vermoegen-erklaerung\\_zur\\_vermoegensverwendung & 0 \\\\\n",
      "VisualPageClassifier & 3 & vermoegen-kontoauszug & 1 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 12 &  &  & \\\\\n",
      "Ground Truth & 4 & rv-altersrente-anpassung & 0\\\\\n",
      "PageComparisionEncoder & 4 & rv-altersrente-anpassung & 1 \\\\\n",
      "VisualPageClassifier & 4 & rv-em\\_rente-rentenbescheid & 0 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 13 &  &  & \\\\\n",
      "Ground Truth & 4 & rv-altersrente-anpassung & 1\\\\\n",
      "PageComparisionEncoder & 5 & rv-altersrente-anpassung & 0 \\\\\n",
      "VisualPageClassifier & 4 & aerztliche\\_unterlagen & 1 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 14 &  &  & \\\\\n",
      "Ground Truth & 5 & einkommen-betriebsrente & 0\\\\\n",
      "PageComparisionEncoder & 5 & auskunftsschreiben\\_jobcenter & 1 \\\\\n",
      "VisualPageClassifier & 4 & aerztliche\\_unterlagen & 2 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 15 &  &  & \\\\\n",
      "Ground Truth & 5 & einkommen-betriebsrente & 1\\\\\n",
      "PageComparisionEncoder & 5 & antrag-sachversicherung & 3 \\\\\n",
      "VisualPageClassifier & 4 & vermoegen-kontoauszug & 3 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 16 &  &  & \\\\\n",
      "Ground Truth & 6 & antrag-ausweiskopie & 0\\\\\n",
      "PageComparisionEncoder & 6 & antrag-auslaenderrechtlicher\\_status & 0 \\\\\n",
      "VisualPageClassifier & 5 & antrag-auslaenderrechtlicher\\_status & 0 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 17 &  &  & \\\\\n",
      "Ground Truth & 7 & kdu-nachweis\\_kuendigung\\_wohnung & 0\\\\\n",
      "PageComparisionEncoder & 0 & antrag-formlos & 2 \\\\\n",
      "VisualPageClassifier & 6 & aerztliche\\_unterlagen & 0 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 18 &  &  & \\\\\n",
      "Ground Truth & 14 & kdu-betriebskosten\\_nebenkosten & 0\\\\\n",
      "PageComparisionEncoder & 5 & mobilitaetshilfe-rechnungen\\_fahrdienstleister & 4 \\\\\n",
      "VisualPageClassifier & 7 & antrag-sachversicherung & 0 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n",
      "Page 19 &  &  & \\\\\n",
      "Ground Truth & 14 & kdu-betriebskosten\\_nebenkosten & 1\\\\\n",
      "PageComparisionEncoder & 5 & antrag-sachversicherung & 0 \\\\\n",
      "VisualPageClassifier & 8 & aerztliche\\_unterlagen & 1 \\\\\n",
      "MultipageTransformer & N/A & N/A & N/A \\\\ \\hline\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, page_gt in enumerate(ground_truth):\n",
    "    print(f'Page {idx +1} &  &  & \\\\\\\\')\n",
    "    print(f'Ground Truth & {page_gt[\"doc_id\"]} & {page_gt[\"doc_class\"]} & {page_gt[\"page_nr\"]}\\\\\\\\'.replace(\"_\", \"\\\\_\").replace(\".\", \"-\"))\n",
    "    print(f'PageComparisionEncoder & {d_id[idx]} & {d_class[idx]} & {d_nr[idx]} \\\\\\\\'.replace(\"_\", \"\\\\_\").replace(\".\", \"-\"))\n",
    "    print(f'VisualPageClassifier & {v_id[idx]} & {v_class[idx]} & {v_nr[idx]} \\\\\\\\'.replace(\"_\", \"\\\\_\").replace(\".\", \"-\"))\n",
    "    print(f'MultipageTransformer & {t_id[idx]} & {t_class[idx]} & {t_nr[idx]} \\\\\\\\ \\\\hline'.replace(\"_\", \"\\\\_\").replace(\".\", \"-\"))\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipage-classifier-training-AUw5wBFA-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac4961f1a54f42faafdec5556e4f4c3e1beb539dc12fb47f919b6f0d50a14ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
